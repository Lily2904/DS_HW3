{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lilit_Kalantar_HW3_Problem2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc4Y_P253dnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installing scrapy\n",
        "!pip install scrapy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIeduahqC67V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time \n",
        "import requests\n",
        "from scrapy.http import TextResponse "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw5ChfOgDFhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://staff.am/en/jobs\"\n",
        "base_url = \"https://staff.am\"\n",
        "page = requests.get(url)\n",
        "response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6TeGKyzRpKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining functions for companies, jobs, deadlines, locations, and individual pages\n",
        "#which scrape the first page of the websited using css and xpath selectors interchangeably\n",
        "#Deadline and location needed some data cleaning, which was perfomed by the codes inside their functions\n",
        "def get_company(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  company = response.css(\"p[class = 'job_list_company_title']::text\").extract()\n",
        "  return company \n",
        "  \n",
        "def get_job(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  job = response.css(\"p[class = 'font_bold']::text\").extract()\n",
        "  return job \n",
        "\n",
        "def get_deadline(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  deadline = response.xpath(\"//div[@class='job-inner job-list-deadline']/p[1]/text()\").extract()\n",
        "  deadline = [i.replace(\"\\n\",\"\") for i in deadline]\n",
        "  deadline_updated = [i for i in deadline if len(i)>0]\n",
        "  return deadline_updated\n",
        "\n",
        "def get_location(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  location = response.css(\"div[class = 'job-inner job-list-deadline'] > p[class = 'job_location']::text\").extract()\n",
        "  location = [i.replace(\"\\n\",\"\") for i in location]\n",
        "  location_updated = [i for i in location if len(i)>0]\n",
        "  return location_updated\n",
        "\n",
        "def get_individual_page(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  individual_page = [base_url + i for i in response.css(\"div[class = 'jobListButtonsBlock job_list_button_block'] > div[class = 'job-inner-right text-right load-more-container pull-right'] > a[class = 'load-more btn width100 job_load_more radius_changes']::attr(href)\").extract()]\n",
        "  return individual_page\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9FJ1KWIBY6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definng all_pages with a range of 12 (number of pages on the website) to use it while scraping all pages\n",
        "all_pages = [\"https://staff.am/en/jobs?page={}&per-page=50\".format(i) for i in range(1,13)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb3b4601BaCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating empty lists for each item and extending them with first scraped page to get the scraped info from all pages\n",
        "all_companies = []\n",
        "all_jobs = []\n",
        "all_deadlines = []\n",
        "all_locations = []\n",
        "all_individual_pages = []\n",
        "for i in all_pages:\n",
        "    all_companies.extend(get_company(i))\n",
        "    all_jobs.extend(get_job(i))\n",
        "    all_deadlines.extend(get_deadline(i))\n",
        "    all_locations.extend(get_location(i))\n",
        "    all_individual_pages.extend(get_individual_page(i))\n",
        "    time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkVJh9eXQbLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combining empty lists and turning them into a DataFrame\n",
        "df = pd.DataFrame(np.column_stack([all_companies, all_jobs, all_deadlines, all_locations, all_individual_pages]), columns = ['Companies', 'Jobs', 'Deadline', 'Location', 'Individual_Page'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggyPbSbNRdRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the results in a csv file\n",
        " df.to_csv('jobs.csv', index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjK5mssMRh7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the new csv file to perform analysis \n",
        "data = pd.read_csv(\"jobs.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWwh8ceaWpLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f6c63978-70b1-4714-e001-20f963b6eba7"
      },
      "source": [
        "# 1. The function value_counts() defines the number of job postings per company and head(5) shows the top 5\n",
        "data.Companies.value_counts().head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Digitain              32\n",
              "SoftConstruct         29\n",
              "PicsArt               23\n",
              "ServiceTitan          16\n",
              "TeamViewer Armenia    12\n",
              "Name: Companies, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l71JGPpcvsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b156ff4-3f5f-42a7-eb72-274d8ab9504e"
      },
      "source": [
        "# 2. As the word \"data\" is mostly used with a \"D\" in job postings, I needed to specify that it's lower case, \n",
        "# then look for all the results containing it in \"Jobs\" column, then count the values and sum them up to get the final number\n",
        "data[data.Jobs.str.lower().str.contains(\"data\")][\"Jobs\"].value_counts().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}